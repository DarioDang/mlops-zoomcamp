{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7749e49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import library \n",
    "import pandas as pd \n",
    "import pickle\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LinearRegression \n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.ensemble import ExtraTreesRegressor, GradientBoostingRegressor, RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "de6bcabb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='mlflow-artifacts:/1', creation_time=1746401851956, experiment_id='1', last_update_time=1746401851956, lifecycle_stage='active', name='nyc-taxi-experiment', tags={}>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mlflow\n",
    "\n",
    "# Set the tracking URI to the same one used in your UI\n",
    "mlflow.set_tracking_uri(\"http://127.0.0.1:5001\")  \n",
    "\n",
    "# Create or set the experiment\n",
    "mlflow.set_experiment(\"nyc-taxi-experiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10004b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataframe(filename):\n",
    "    \"Write a function to read and preprocessing data\"\n",
    "    # Read the dataset\n",
    "    df_taxi = pd.read_parquet(filename)\n",
    "\n",
    "    # Adjust dropoff & pickup to pandas datetime \n",
    "    df_taxi['lpep_pickup_datetime'] = pd.to_datetime(df_taxi.lpep_pickup_datetime)\n",
    "    df_taxi['lpep_dropoff_datetime'] = pd.to_datetime(df_taxi.lpep_dropoff_datetime)\n",
    "    \n",
    "    # Calculate the duration (drop_off -  pick_up)\n",
    "    df_taxi['duration'] = df_taxi.lpep_dropoff_datetime - df_taxi.lpep_pickup_datetime\n",
    "    \n",
    "    # Adjust the duration in minutes for prediction \n",
    "    df_taxi['duration_minutes'] = df_taxi['duration'].dt.total_seconds() / 60\n",
    "\n",
    "    # Since there are a lot of duration less than 1 minutes. We filter only duration between 1 minutes to 99% percentile\n",
    "    df_taxi = df_taxi[(df_taxi['duration_minutes'] >= 1) & (df_taxi['duration_minutes'] <= 60)]\n",
    "\n",
    "    # Feature Engineering \n",
    "    categorical_variables = ['PULocationID', 'DOLocationID']\n",
    "    numerical_variables = ['trip_distance']\n",
    "\n",
    "    # Convert it into \"str\"\n",
    "    df_taxi[categorical_variables] = df_taxi[categorical_variables].astype(str)\n",
    "    \n",
    "    return df_taxi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa53e8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = read_dataframe('../00-Dataset/green_tripdata_2021-01.parquet')\n",
    "df_val = read_dataframe('../00-Dataset/green_tripdata_2021-02.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d159ac24",
   "metadata": {},
   "source": [
    "### Create the training pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e45c7cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering \n",
    "categorical_variables = ['PULocationID', 'DOLocationID']\n",
    "numerical_variables = ['trip_distance']\n",
    "\n",
    "# Vectorizer the training variables \n",
    "dv = DictVectorizer()\n",
    "\n",
    "# Convert it into dictionary \n",
    "train_dicts = df_train[categorical_variables + numerical_variables].to_dict(orient = 'records')\n",
    "X_train = dv.fit_transform(train_dicts)\n",
    "\n",
    "# Create the validation set \n",
    "val_dicts = df_val[categorical_variables + numerical_variables].to_dict(orient = 'records')\n",
    "X_val = dv.transform(val_dicts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb38cda",
   "metadata": {},
   "source": [
    "### Try To Combine the input features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98fb552c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['PU_DO'] = df_train['PULocationID'] + '_' + df_train['DOLocationID']\n",
    "df_val['PU_DO'] = df_val['PULocationID'] + '_' + df_val['DOLocationID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85cf46bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering \n",
    "categorical_variables = ['PU_DO']  #['PULocationID', 'DOLocationID']\n",
    "numerical_variables = ['trip_distance']\n",
    "\n",
    "# Vectorizer the training variables \n",
    "dv = DictVectorizer()\n",
    "\n",
    "# Convert it into dictionary \n",
    "train_dicts = df_train[categorical_variables + numerical_variables].to_dict(orient = 'records')\n",
    "X_train = dv.fit_transform(train_dicts)\n",
    "\n",
    "# Create the validation set \n",
    "val_dicts = df_val[categorical_variables + numerical_variables].to_dict(orient = 'records')\n",
    "X_val = dv.transform(val_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5520b565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the Prediction_Variables \n",
    "predictor = 'duration_minutes'\n",
    "y_train = df_train[predictor].values\n",
    "y_val = df_val[predictor].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1dc02ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"LinearSVR\": LinearSVR(),\n",
    "    \"ExtraTrees\": ExtraTreesRegressor(n_jobs=-1),\n",
    "    \"GradientBoosting\": GradientBoostingRegressor(),\n",
    "    \"RandomForest\": RandomForestRegressor(n_jobs=-1),\n",
    "    \"XGBoost\": XGBRegressor(n_jobs=-1)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d4685eca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "2025/05/07 02:53:53 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /tmp/tmprxz14t5j/model/model.pkl, flavor: sklearn). Fall back to return ['scikit-learn==1.0.2', 'cloudpickle==2.0.0']. Set logging level to DEBUG to see the full traceback. \n",
      "\u001b[31m2025/05/07 02:53:53 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVR RMSE: 779.25\n",
      "ðŸƒ View run LinearSVR at: http://127.0.0.1:5001/#/experiments/1/runs/e644e8399b884accb593474b33f46c17\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5001/#/experiments/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/05/07 03:17:26 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /tmp/tmpw5fdqvvq/model/model.pkl, flavor: sklearn). Fall back to return ['scikit-learn==1.0.2', 'cloudpickle==2.0.0']. Set logging level to DEBUG to see the full traceback. \n",
      "\u001b[31m2025/05/07 03:17:26 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExtraTrees RMSE: 6.94\n",
      "ðŸƒ View run ExtraTrees at: http://127.0.0.1:5001/#/experiments/1/runs/a221c5fad4974fefa4f84b818db00f4c\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5001/#/experiments/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/05/07 03:17:46 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /tmp/tmpcjakjkwt/model/model.pkl, flavor: sklearn). Fall back to return ['scikit-learn==1.0.2', 'cloudpickle==2.0.0']. Set logging level to DEBUG to see the full traceback. \n",
      "\u001b[31m2025/05/07 03:17:46 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradientBoosting RMSE: 6.74\n",
      "ðŸƒ View run GradientBoosting at: http://127.0.0.1:5001/#/experiments/1/runs/07ce6eb2e69546739b59e9f8780059f8\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5001/#/experiments/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/05/07 03:25:51 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /tmp/tmpxc5etcjz/model/model.pkl, flavor: sklearn). Fall back to return ['scikit-learn==1.0.2', 'cloudpickle==2.0.0']. Set logging level to DEBUG to see the full traceback. \n",
      "\u001b[31m2025/05/07 03:25:51 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForest RMSE: 6.91\n",
      "ðŸƒ View run RandomForest at: http://127.0.0.1:5001/#/experiments/1/runs/0dbdbb47826d48879214c89be7a09c5f\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5001/#/experiments/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/05/07 03:26:05 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /tmp/tmpirm71iil/model/model.pkl, flavor: sklearn). Fall back to return ['scikit-learn==1.0.2', 'cloudpickle==2.0.0']. Set logging level to DEBUG to see the full traceback. \n",
      "\u001b[31m2025/05/07 03:26:05 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost RMSE: 6.64\n",
      "ðŸƒ View run XGBoost at: http://127.0.0.1:5001/#/experiments/1/runs/b329f82bc08e4d5ebaf62f74833d5c54\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5001/#/experiments/1\n"
     ]
    }
   ],
   "source": [
    "for name, model in models.items():\n",
    "    with mlflow.start_run(run_name=name):\n",
    "        #set the tag name for who response\n",
    "        mlflow.set_tag(\"developer\",\"Dario\")\n",
    "        \n",
    "        # Train the model\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_val)\n",
    "        rmse = mean_squared_error(y_val, y_pred, squared=False)\n",
    "        \n",
    "        # Log parameters and metrics\n",
    "        mlflow.set_tag(\"model\", name)\n",
    "        mlflow.log_param(\"train_rows\", X_train.shape[0])\n",
    "        mlflow.log_metric(\"rmse\", rmse)\n",
    "        \n",
    "        # save the preprocessing \n",
    "        with open(\"models/preprocessor.b\", \"wb\") as f_out:\n",
    "            pickle.dump(dv, f_out)\n",
    "        \n",
    "        # log the preprocessing step \n",
    "        mlflow.log_artifact(\"models/preprocessor.b\", artifact_path = \"preprocessor\")\n",
    "        \n",
    "        # Log model itself\n",
    "        mlflow.sklearn.log_model(model, artifact_path=\"model\")\n",
    "        \n",
    "        print(f\"{name} RMSE: {rmse:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c648fed",
   "metadata": {},
   "source": [
    "## Use the ML Client class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9258233f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/05/07 01:06:08 INFO mlflow.store.db.utils: Creating initial MLflow database tables...\n",
      "2025/05/07 01:06:08 INFO mlflow.store.db.utils: Updating database tables\n",
      "INFO  [alembic.runtime.migration] Context impl SQLiteImpl.\n",
      "INFO  [alembic.runtime.migration] Will assume non-transactional DDL.\n",
      "INFO  [alembic.runtime.migration] Running upgrade  -> 451aebb31d03, add metric step\n",
      "INFO  [alembic.runtime.migration] Running upgrade 451aebb31d03 -> 90e64c465722, migrate user column to tags\n",
      "INFO  [alembic.runtime.migration] Running upgrade 90e64c465722 -> 181f10493468, allow nulls for metric values\n",
      "INFO  [alembic.runtime.migration] Running upgrade 181f10493468 -> df50e92ffc5e, Add Experiment Tags Table\n",
      "INFO  [alembic.runtime.migration] Running upgrade df50e92ffc5e -> 7ac759974ad8, Update run tags with larger limit\n",
      "INFO  [alembic.runtime.migration] Running upgrade 7ac759974ad8 -> 89d4b8295536, create latest metrics table\n",
      "INFO  [89d4b8295536_create_latest_metrics_table_py] Migration complete!\n",
      "INFO  [alembic.runtime.migration] Running upgrade 89d4b8295536 -> 2b4d017a5e9b, add model registry tables to db\n",
      "INFO  [2b4d017a5e9b_add_model_registry_tables_to_db_py] Adding registered_models and model_versions tables to database.\n",
      "INFO  [2b4d017a5e9b_add_model_registry_tables_to_db_py] Migration complete!\n",
      "INFO  [alembic.runtime.migration] Running upgrade 2b4d017a5e9b -> cfd24bdc0731, Update run status constraint with killed\n",
      "INFO  [alembic.runtime.migration] Running upgrade cfd24bdc0731 -> 0a8213491aaa, drop_duplicate_killed_constraint\n",
      "INFO  [alembic.runtime.migration] Running upgrade 0a8213491aaa -> 728d730b5ebd, add registered model tags table\n",
      "INFO  [alembic.runtime.migration] Running upgrade 728d730b5ebd -> 27a6a02d2cf1, add model version tags table\n",
      "INFO  [alembic.runtime.migration] Running upgrade 27a6a02d2cf1 -> 84291f40a231, add run_link to model_version\n",
      "INFO  [alembic.runtime.migration] Running upgrade 84291f40a231 -> a8c4a736bde6, allow nulls for run_id\n",
      "INFO  [alembic.runtime.migration] Running upgrade a8c4a736bde6 -> 39d1c3be5f05, add_is_nan_constraint_for_metrics_tables_if_necessary\n",
      "INFO  [alembic.runtime.migration] Running upgrade 39d1c3be5f05 -> c48cb773bb87, reset_default_value_for_is_nan_in_metrics_table_for_mysql\n",
      "INFO  [alembic.runtime.migration] Running upgrade c48cb773bb87 -> bd07f7e963c5, create index on run_uuid\n",
      "INFO  [alembic.runtime.migration] Running upgrade bd07f7e963c5 -> 0c779009ac13, add deleted_time field to runs table\n",
      "INFO  [alembic.runtime.migration] Running upgrade 0c779009ac13 -> cc1f77228345, change param value length to 500\n",
      "INFO  [alembic.runtime.migration] Running upgrade cc1f77228345 -> 97727af70f4d, Add creation_time and last_update_time to experiments table\n",
      "INFO  [alembic.runtime.migration] Running upgrade 97727af70f4d -> 3500859a5d39, Add Model Aliases table\n",
      "INFO  [alembic.runtime.migration] Running upgrade 3500859a5d39 -> 7f2a7d5fae7d, add datasets inputs input_tags tables\n",
      "INFO  [alembic.runtime.migration] Running upgrade 7f2a7d5fae7d -> 2d6e25af4d3e, increase max param val length from 500 to 8000\n",
      "INFO  [alembic.runtime.migration] Running upgrade 2d6e25af4d3e -> acf3f17fdcc7, add storage location field to model versions\n",
      "INFO  [alembic.runtime.migration] Running upgrade acf3f17fdcc7 -> 867495a8f9d4, add trace tables\n",
      "INFO  [alembic.runtime.migration] Running upgrade 867495a8f9d4 -> 5b0e9adcef9c, add cascade deletion to trace tables foreign keys\n",
      "INFO  [alembic.runtime.migration] Running upgrade 5b0e9adcef9c -> 4465047574b1, increase max dataset schema size\n",
      "INFO  [alembic.runtime.migration] Running upgrade 4465047574b1 -> f5a4f2784254, increase run tag value limit to 8000\n",
      "INFO  [alembic.runtime.migration] Running upgrade f5a4f2784254 -> 0584bdc529eb, add cascading deletion to datasets from experiments\n",
      "INFO  [alembic.runtime.migration] Context impl SQLiteImpl.\n",
      "INFO  [alembic.runtime.migration] Will assume non-transactional DDL.\n"
     ]
    }
   ],
   "source": [
    "# import the mlflow client library\n",
    "from mlflow.tracking import MlflowClient \n",
    "\n",
    "# set the tracking URI\n",
    "MLFLOW_TRACKING_URI = \"sqlite:///mflow.db\"\n",
    "\n",
    "client = MlflowClient(tracking_uri = MLFLOW_TRACKING_URI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "00a255b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: nyc-taxi-experiment, ID: 1\n",
      "Name: Default, ID: 0\n"
     ]
    }
   ],
   "source": [
    "# Listing all the experiments\n",
    "client = MlflowClient()\n",
    "experiments = client.search_experiments()\n",
    "for exp in experiments:\n",
    "    print(f\"Name: {exp.name}, ID: {exp.experiment_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4ba39bc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run_id: 1f3bdd5050ec470d84fb45ee81d7c57e, rmse:6.3012\n",
      "run_id: 554c31ab5af245adb74b7857b90137a2, rmse:6.3012\n",
      "run_id: fcbb9a4fe93b4071abe4e2f1ae5dee6b, rmse:6.3012\n",
      "run_id: 7ec765ee6b9b485b893f5c366800c424, rmse:6.3026\n",
      "run_id: 7117ea3740a943d5aeec22f440418050, rmse:6.3081\n"
     ]
    }
   ],
   "source": [
    "from mlflow.entities import ViewType \n",
    "# Show the best run models\n",
    "runs = client.search_runs(\n",
    "    experiment_ids = 1,\n",
    "    filter_string = \"metrics.rmse < 7\",\n",
    "    run_view_type = ViewType.ACTIVE_ONLY,\n",
    "    max_results = 5, \n",
    "    order_by = [\"metrics.rmse ASC\"]\n",
    ")\n",
    "\n",
    "# show the result\n",
    "for run in runs:\n",
    "    print(f\"run_id: {run.info.run_id}, rmse:{run.data.metrics['rmse']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d541289",
   "metadata": {},
   "source": [
    "### Promote models to model registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "288e11fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registered model 'nyc-taxi-regressor' already exists. Creating a new version of this model...\n",
      "2025/05/07 03:31:49 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: nyc-taxi-regressor, version 6\n",
      "Created version '6' of model 'nyc-taxi-regressor'.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ModelVersion: aliases=[], creation_timestamp=1746588709103, current_stage='None', description='', last_updated_timestamp=1746588709103, name='nyc-taxi-regressor', run_id='b329f82bc08e4d5ebaf62f74833d5c54', run_link='', source='mlflow-artifacts:/1/b329f82bc08e4d5ebaf62f74833d5c54/artifacts/model', status='READY', status_message=None, tags={}, user_id='', version='6'>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mlflow\n",
    "\n",
    "# Set the correct tracking URI\n",
    "mlflow.set_tracking_uri(\"http://127.0.0.1:5001\")  # or your actual URI\n",
    "\n",
    "# Use correct run ID and model URI\n",
    "run_id = \"b329f82bc08e4d5ebaf62f74833d5c54\"\n",
    "model_uri = f\"runs:/{run_id}/model\"\n",
    "\n",
    "# Register the model\n",
    "mlflow.register_model(model_uri=model_uri, name=\"nyc-taxi-regressor\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d9cc96",
   "metadata": {},
   "source": [
    "### Transition model to another stage "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "09535037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "version: 2, stage: Staging\n",
      "version: 3, stage: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_11746/151408037.py:3: FutureWarning: ``mlflow.tracking.client.MlflowClient.get_latest_versions`` is deprecated since 2.9.0. Model registry stages will be removed in a future major release. To learn more about the deprecation of model registry stages, see our migration guide here: https://mlflow.org/docs/latest/model-registry.html#migrating-from-stages\n",
      "  lastest_versions = client.get_latest_versions(name = model_name)\n"
     ]
    }
   ],
   "source": [
    "# get the lastest verion \n",
    "model_name = \"nyc-taxi-regressor\"\n",
    "lastest_versions = client.get_latest_versions(name = model_name)\n",
    "\n",
    "for version in lastest_versions:\n",
    "    print(f\"version: {version.version}, stage: {version.current_stage}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8f440a96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_11746/2384279098.py:4: FutureWarning: ``mlflow.tracking.client.MlflowClient.transition_model_version_stage`` is deprecated since 2.9.0. Model registry stages will be removed in a future major release. To learn more about the deprecation of model registry stages, see our migration guide here: https://mlflow.org/docs/latest/model-registry.html#migrating-from-stages\n",
      "  client.transition_model_version_stage(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ModelVersion: aliases=[], creation_timestamp=1746588475883, current_stage='Production', description='', last_updated_timestamp=1746588768798, name='nyc-taxi-regressor', run_id='b329f82bc08e4d5ebaf62f74833d5c54', run_link='', source='mlflow-artifacts:/1/b329f82bc08e4d5ebaf62f74833d5c54/artifacts/model', status='READY', status_message=None, tags={'model': 'xgboost'}, user_id='', version='5'>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_version = 5\n",
    "new_stage = \"Production\"\n",
    "# transition model stage\n",
    "client.transition_model_version_stage(\n",
    "    name = model_name,\n",
    "    version = model_version, \n",
    "    stage = new_stage,\n",
    "    archive_existing_versions = False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368a6ebb",
   "metadata": {},
   "source": [
    "### Change the model description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "4bf820dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ModelVersion: aliases=[], creation_timestamp=1746588475883, current_stage='Production', description='The model version 5 was transition to Production on 2025-05-07', last_updated_timestamp=1746588789617, name='nyc-taxi-regressor', run_id='b329f82bc08e4d5ebaf62f74833d5c54', run_link='', source='mlflow-artifacts:/1/b329f82bc08e4d5ebaf62f74833d5c54/artifacts/model', status='READY', status_message=None, tags={'model': 'xgboost'}, user_id='', version='5'>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "date = datetime.today().date()\n",
    "client.update_model_version(\n",
    "    name = model_name,\n",
    "    version = model_version, \n",
    "    description = f\"The model version {model_version} was transition to {new_stage} on {date}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "79a4853a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_11746/683197068.py:5: FutureWarning: ``mlflow.tracking.client.MlflowClient.transition_model_version_stage`` is deprecated since 2.9.0. Model registry stages will be removed in a future major release. To learn more about the deprecation of model registry stages, see our migration guide here: https://mlflow.org/docs/latest/model-registry.html#migrating-from-stages\n",
      "  client.transition_model_version_stage(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ModelVersion: aliases=[], creation_timestamp=1746579393292, current_stage='Archived', description='The model version 2 was transition to Archived on 2025-05-07', last_updated_timestamp=1746588823712, name='nyc-taxi-regressor', run_id='93589ab26598432588ea953f005931bb', run_link='', source='mlflow-artifacts:/1/93589ab26598432588ea953f005931bb/artifacts/model', status='READY', status_message=None, tags={'model': 'gradientboostingregrssor'}, user_id='', version='2'>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Change model 2 to production\n",
    "model_version = 2\n",
    "new_stage = \"Archived\"\n",
    "# transition model stage\n",
    "client.transition_model_version_stage(\n",
    "    name = model_name,\n",
    "    version = model_version, \n",
    "    stage = new_stage,\n",
    "    archive_existing_versions = False\n",
    ")\n",
    "\n",
    "date = datetime.today().date()\n",
    "client.update_model_version(\n",
    "    name = model_name,\n",
    "    version = model_version, \n",
    "    description = f\"The model version {model_version} was transition to {new_stage} on {date}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e04374",
   "metadata": {},
   "source": [
    "## Building the automation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "85d8e3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataframe(filename):\n",
    "    \"Write a function to read and preprocessing data\"\n",
    "    # Read the dataset\n",
    "    df_taxi = pd.read_parquet(filename)\n",
    "\n",
    "    # Adjust dropoff & pickup to pandas datetime \n",
    "    df_taxi['lpep_pickup_datetime'] = pd.to_datetime(df_taxi.lpep_pickup_datetime)\n",
    "    df_taxi['lpep_dropoff_datetime'] = pd.to_datetime(df_taxi.lpep_dropoff_datetime)\n",
    "    \n",
    "    # Calculate the duration (drop_off -  pick_up)\n",
    "    df_taxi['duration'] = df_taxi.lpep_dropoff_datetime - df_taxi.lpep_pickup_datetime\n",
    "    \n",
    "    # Adjust the duration in minutes for prediction \n",
    "    df_taxi['duration_minutes'] = df_taxi['duration'].dt.total_seconds() / 60\n",
    "\n",
    "    # Since there are a lot of duration less than 1 minutes. We filter only duration between 1 minutes to 99% percentile\n",
    "    df_taxi = df_taxi[(df_taxi['duration_minutes'] >= 1) & (df_taxi['duration_minutes'] <= 60)]\n",
    "\n",
    "    # Feature Engineering \n",
    "    categorical_variables = ['PULocationID', 'DOLocationID']\n",
    "    numerical_variables = ['trip_distance']\n",
    "\n",
    "    # Convert it into \"str\"\n",
    "    df_taxi[categorical_variables] = df_taxi[categorical_variables].astype(str)\n",
    "    return df_taxi\n",
    "\n",
    "\n",
    "\n",
    "def preprocess(df, dv):\n",
    "    \"Write the function to preprocessing data\"\n",
    "    df['PU_DO'] = df['PULocationID'] + '_' + df['DOLocationID']\n",
    "    categorical = ['PU_DO']\n",
    "    numerical = numerical_variables\n",
    "    train_dicts = df[categorical + numerical].to_dict(orient = 'records')\n",
    "    return dv.transform(train_dicts)\n",
    "\n",
    "\n",
    "\n",
    "def test_model(name, stage, X_test, y_test):\n",
    "    \"Write a function to test the model\"\n",
    "    model = mlflow.pyfunc.load_model(f\"models:/{name}/{stage}\")\n",
    "    y_pred = model.predict(X_test)\n",
    "    return {\"rmse\": mean_squared_error(y_test, y_pred, squared = False)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "06114d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the dataframe\n",
    "df = read_dataframe('../00-Dataset/green_tripdata_2021-03.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "06087e25",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afc934b47e4c48dfaf0c6247cc298e83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'/workspaces/mlops-zoomcamp/02-experiment-tracking/preprocessor'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download the preprocessor artifact to preprocess training data \n",
    "run_id = \"b329f82bc08e4d5ebaf62f74833d5c54\"\n",
    "client.download_artifacts(run_id = run_id, path = 'preprocessor', dst_path = '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "cb291510",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the preprocessot to transform new data (e.g: DictVectorize)\n",
    "import pickle \n",
    "\n",
    "with open(\"preprocessor/preprocessor.b\", \"rb\") as f_in:\n",
    "    dv = pickle.load(f_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7f5201de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess the testing dataset\n",
    "X_test = preprocess(df,dv)\n",
    "\n",
    "# define the target variable\n",
    "target = \"duration_minutes\"\n",
    "y_test = df[target].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2470ed46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15635fecaeef47aa81f5148df6f149c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 566 ms, sys: 16.8 ms, total: 583 ms\n",
      "Wall time: 481 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'rmse': 6.57711317277908}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run the model with production stage\n",
    "%time test_model(name = model_name, stage = 'Production', X_test = X_test, y_test = y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1994c954",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c34444d8c2a4f37b00c3d503127e34d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.5 s, sys: 3.77 s, total: 15.2 s\n",
      "Wall time: 16.4 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'rmse': 6.88109797526315}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run the model with stagging stage\n",
    "%time test_model(name = model_name, stage = 'Staging', X_test = X_test, y_test = y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f4e4d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
